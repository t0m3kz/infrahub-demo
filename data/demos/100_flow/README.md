# Inter-Topology Connectivity Flow: When "Just Connect Everything" Becomes a Religion

*Where Enterprise IT Learns That Redundancy Isn't Just a Buzzword‚ÄîIt's What Prevents Career-Ending Outages*

## Overview - The Multi-Cloud Dream That Actually Works (Surprisingly)

This demo showcases what happens when someone in management says "We need multi-cloud with full redundancy" and then actually funds it properly for once. A rare sight in the wild, like spotting a unicorn or a project that finishes on time and under budget.

**The Journey No Packet Should Have to Make (But They Do):**

```
Enterprise DC (DC1) - Where Legacy Systems Go to Retire Slowly
    ‚Üì [Dark Fiber - Because We Have Money]
Colocation Sites (Equinix) - Fancy Name for "Renting Expensive Closets"
    ‚Üì [Direct Connect - AWS's Premium Toll Road]
Cloud Provider (AWS) - Where Dreams of Cost Savings Go to Die
```

**Architecture Philosophy:** When your VP of Infrastructure attended that one AWS conference and came back saying "we need cloud connectivity," but the CTO actually approved a budget that makes sense. This is what engineering excellence looks like when management accidentally does the right thing.

---

## The Grand Connectivity Scheme (N+2 Redundancy: Because N+1 Is For Cowards)

### Multi-Site Strategy - The "What Could Possibly Go Wrong?" Approach

- **Primary Data Center (DC1)**: Frankfurt üá©üá™ - Your on-premises castle, complete with moat
- **Colocation Site 1 (FR5)**: Equinix Frankfurt - The expensive hotel for your routers
- **Colocation Site 2 (AM5)**: Equinix Amsterdam - The backup expensive hotel (with better stroopwafels)
- **Cloud Region 1**: AWS eu-central-1 Frankfurt - Where your money goes to disappear in 1ms latency
- **Cloud Region 2**: AWS eu-west-1 Ireland - The backup region (because Brexit wasn't confusing enough)

**Total Devices:** 6 edge/border routers (because someone finally read the architecture best practices)

**Total Circuits:** 14 physical + 15 virtual = 29 ways your budget got approved and your competitors are jealous

---

## Architecture Components (The "We Actually Did This Right" Edition)

### Layered Connectivity Model

This demo follows a **layered connectivity model** that separates physical infrastructure from logical services:

1. **Physical Layer** (`DcimCable`): Actual fiber/copper connections between device interfaces
2. **Circuit Layer** (`TopologyCircuit`): Service definitions on physical connections (bandwidth, provider, SLAs)
3. **Virtual Layer** (`TopologyVirtualLink`): Overlay tunnels and cloud connections
4. **Service Layer** (`ManagedCircuitService`, `ManagedVirtualLinkService`): Interface bindings and configurations

**Why This Matters:** You can have physical cables (dark fiber) without circuits (not yet lit), circuits without virtual links (pure L1), or virtual links without physical cables (cloud-to-cloud). Each layer builds on the previous one.

### Connection Types - When Finance Asks "Do We Really Need All These?"

#### 1. DC ‚Üí Colocations: Dark Fiber (The Rolls-Royce of Connectivity)

**Physical Layer:** `DcimCable` with type `smf` (Single-Mode Fiber)
**Circuit Layer:** `TopologyCircuit` with type `dark_fiber`

**What This Means:** We literally have our own fiber strands between locations. Yes, it's expensive. No, we don't share with others. Yes, finance questioned it. No, we don't care‚Äîit's 100Gbps of pure, uncontested glory.

**Physical Cables:**
- **DC1 ‚Üî FR5**: 2√ó SMF cables (dc1-border-01/02 to fr5-edge-01/02)
- **DC1 ‚Üî AM5**: 2√ó SMF cables (dc1-border-01/02 to am5-edge-01/02)

**Circuits:**
- **DC1 ‚Üí FR5**: 2√ó 100G dark fiber (DF-FRA-001, DF-FRA-002)
- **DC1 ‚Üí AM5**: 2√ó 100G dark fiber (DF-AMS-001, DF-AMS-002)

**Termination:** Border routers (DC1) ‚Üî Edge routers (Colocation)

**Justification to Management:** "Do you want to explain to customers why our 'multi-site redundant architecture' fell over when a construction crew in Frankfurt hit a single fiber?"

#### 2. Colocations ‚Üí Cloud: Direct Connect (AWS's Answer to "How Much Can We Charge?")

**Implementation:** `TopologyVirtualLink` with type `direct_connect_aws`

**What This Means:** We're paying AWS premium prices for "dedicated" connectivity instead of screaming across the public internet like peasants. Worth it? When the alternative is explaining latency spikes during earnings calls, absolutely.

- **FR5 ‚Üí AWS Frankfurt**: 2√ó 10G Direct Connect (Primary + Backup)
- **AM5 ‚Üí AWS Ireland**: 2√ó 10G Direct Connect (Primary + Backup)
- **FR5 ‚Üí AWS Ireland**: 1√ó 10G Cross-region failover (For when Frankfurt is literally on fire)

**Physical Layer:** `TopologyCircuit` with type `cross_connect` (The $500/month cable everyone forgets about)

**Termination:** PoP router interfaces ‚Üí Cloud VGW/Direct Connect Gateway

**Fun Fact:** Each Direct Connect connection costs more per month than your first car. But hey, sub-5ms latency to S3!

#### 3. Inter-Colocation: Metro Ethernet (When You Need Cities to Talk)

**Physical Layer:** None - Equinix Metro Connect is a **Layer 2 service**
**Circuit Layer:** `TopologyCircuit` with type `metro_ethernet`
**Overlay Layer:** `TopologyVirtualLink` with types `vxlan` and `gre`

**What This Means:** Frankfurt and Amsterdam are now best friends at Layer 2 and Layer 3. They share everything‚Äîroutes, VLANs, and the collective disappointment when Belgium is having "infrastructure issues."

**Important:** There are **NO physical cables** between FR5 and AM5. Equinix Metro Connect is a managed Layer 2 service over Equinix's internal backbone. You don't have access to or control over the physical infrastructure - you just get a VLAN between your cages in different metros.

**Circuits:**
- **FR5 ‚Üî AM5**: 2√ó 100G Equinix Metro Connect (EQX-METRO-001, EQX-METRO-002)

**Overlays:**
- **VXLAN**: Primary + Backup (For when you need to stretch L2 across countries like a bad TRON sequel)
- **GRE Tunnel**: For old-school routing nerds who don't trust fancy overlays

**Use Case:** Inter-site failover so fast your monitoring tools think it's a false alarm

**Marketing Says:** "Seamless multi-site active-active architecture"
**Reality Is:** "Two routers screaming at each other across 350km of Equinix's backbone hoping for sub-3ms RTT"

#### 4. Internet Backup: DIA Circuits (For When Everything Else Burns Down)

**Implementation:**
- Physical: `TopologyCircuit` with type `dia` (Dedicated Internet Access - the "Oh Crap" button)
- Overlay: `TopologyVirtualLink` with type `vpn_ipsec` (Encrypted shouting matches)

**What This Means:** When both Direct Connect circuits fail, both Metro Ethernet paths are down, and you're questioning your career choices, these humble 10G internet connections become your saviors.

- **FR5 ‚Üí Internet**: 10G DIA (Plus IPsec VPN to AWS)
- **AM5 ‚Üí Internet**: 10G DIA (Plus IPsec VPN to AWS)
- **DC1 ‚Üí Internet**: Emergency VPN tunnel (For "break glass in case of total WAN failure" scenarios)

**Latency:** ~20-30ms (compared to DX's 2-5ms)
**Reliability:** Surprisingly good
**Cost:** Fraction of Direct Connect
**Status:** The backup plan everyone mocks until it's the only thing working

---

## The Topology Hierarchy (Nested Like Russian Dolls, But With More BGP)

### Physical Infrastructure Breakdown

#### DC1: Enterprise Data Center (Frankfurt)
**Type:** `TopologyDataCenter`

**Personality:** The mothership. Legacy applications, compliance requirements, and enough security to make getting coffee take 3 badge swipes.

**Devices:**
- `dc1-border-01`: Primary border router (Carries production like a champ)
- `dc1-border-02`: Secondary border router (Backup that never feels appreciated)

**Uplinks:**
- 2√ó 100G ‚Üí FR5 (Dark fiber, because we're fancy)
- 2√ó 100G ‚Üí AM5 (Geographic diversity or paranoia, you decide)
- 1√ó Emergency VPN (For apocalypse scenarios)

**AS Number:** 65000 (Private AS, because we're not THAT fancy)

---

#### Equinix Frankfurt (FR5): The German Engineering Experience

**Type:** `TopologyColocationAZ`
**Hierarchy:** `TopologyColocation` ‚Üí `Metro: Frankfurt` ‚Üí `AZ: FR5`

**Location:** Equinix FR5, Frankfurt, Germany üá©üá™
**Philosophy:** German engineering meets American colocation pricing

**Why Frankfurt?**
- DE-CIX (World's largest internet exchange by traffic)
- Low latency to everywhere in Europe
- German reliability (their trains might be late, but the data center never sleeps)
- Close to AWS eu-central-1 (Like, REALLY close‚Äî0.5ms close)

**Devices:**
- `fr5-edge-01`: Primary edge router (The workhorse)
- `fr5-edge-02`: Secondary edge router (Always ready, occasionally used)

**Connectivity Buffet:**
- 2√ó 100G ‚Üê DC1 (Dark fiber‚Äîthe Autobahn of data)
- 2√ó 10G ‚Üí AWS Frankfurt (Direct Connect‚Äîexpensive but worth it)
- 1√ó 10G ‚Üí AWS Ireland (Cross-region: for when primary AWS region needs a vacation)
- 1√ó 10G ‚Üí Internet (DIA: the "oh shit" backup)
- 2√ó 100G ‚Üî AM5 (Metro Ethernet: fastest way to Amsterdam not involving tulips)

**AS Number:** 65100 (Independent AS because we do eBGP properly)
```
**AS Number:** 65100 (Independent AS because we do eBGP properly)

---

#### Equinix Amsterdam (AM5): The Dutch Connectivity Experience

**Type:** `TopologyColocationAZ`
**Hierarchy:** `TopologyColocation` ‚Üí `Metro: Amsterdam` ‚Üí `AZ: AM5`

**Location:** Equinix AM5, Amsterdam, Netherlands üá≥üá±
**Philosophy:** Where pragmatic Dutch infrastructure meets American venture capital pricing

**Why Amsterdam?**
- AMS-IX (One of the world's largest internet exchanges)
- Geographic diversity from Frankfurt (350km means different fiber paths, different failure domains)
- Dutch reliability (If they can keep the country from flooding, they can keep your packets flowing)
- Direct path to AWS eu-west-1 Ireland (For when you need failover to the Emerald Isle)

**Devices:**
- `am5-edge-01`: Primary edge router (Handles more traffic than anticipated)
- `am5-edge-02`: Secondary edge router (Appreciates being utilized)

**Connectivity Smorgasbord:**
- 2√ó 100G ‚Üê DC1 (Dark fiber‚Äîyour monthly budget's worst nightmare, your uptime's best friend)
- 2√ó 10G ‚Üí AWS Ireland (Direct Connect‚Äîbecause crossing the North Sea in microseconds is cool)
- 1√ó 10G ‚Üí Internet (DIA: insurance policy)
- 2√ó 100G ‚Üî FR5 (Metro Ethernet: the scenic route through Benelux)

**AS Number:** 65101 (Separate from Frankfurt because eBGP between sites is how adults do networking)

---

#### AWS eu-central-1: Cloud Region 1 (Frankfurt)

**Type:** `TopologyCloudRegion`
**Hierarchy:** `TopologyCloud: AWS` ‚Üí `Region: eu-central-1` ‚Üí `Zones: 1a, 1b`

**Location:** Frankfurt, Germany üá©üá™ (Probably‚ÄîAWS doesn't tell you exactly, which is delightfully vague)

**Role:** Primary Cloud Region (Where your workloads pretend to be "on-premises" but with AWS pricing)

**Why This Region?**
- Co-located with DC1 (Same city, which means <1ms latency when Direct Connect works)
- GDPR compliance without the UK's post-Brexit confusion
- Low latency to rest of Europe
- German data sovereignty laws mean your data is REALLY protected

**Connectivity:**
- 2√ó 10G ‚Üê FR5 (Direct Connect: Primary + Backup)
- 1√ó 10G ‚Üê FR5 (VPN over Internet: The "everything is on fire" backup)
- 1√ó Emergency VPN ‚Üê DC1 (Because sometimes you just need to bypass everything)
- 1√ó Transit Gateway ‚Üí eu-west-1 (Inter-region backbone)

**AS Number:** 64512 (AWS's default‚Äîbecause creativity in AS numbering is not their strength)

**Fun Fact:** This is where your Reserved Instances live, wondering why they're underutilized.

---

#### AWS eu-west-1: Cloud Region 2 (Ireland)

**Type:** `TopologyCloudRegion`
**Hierarchy:** `TopologyCloud: AWS` ‚Üí `Region: eu-west-1` ‚Üí `Zones: 1a, 1b`

**Location:** Dublin, Ireland üáÆüá™ (The land of low corporate tax rates and high networking costs)

**Role:** Secondary Cloud Region / DR Site (Where your DR plan lives, hoping it never gets tested)

**Why This Region?**
- Geographic diversity (North Sea between FR and IE means different earthquake zones‚Äîyes, this matters)
- Independent power grid (Irish wind farms vs German nuclear‚Äîdiversification!)
- Different AWS availability zones (For when eu-central-1 is having a bad day)
- English-speaking support (If you need to troubleshoot at 3 AM, accents matter)

**Connectivity:**
- 2√ó 10G ‚Üê AM5 (Direct Connect: Primary + Backup)
- 1√ó 10G ‚Üê FR5 (Cross-region Direct Connect: For maximum paranoia)
- 1√ó 10G ‚Üê AM5 (VPN over Internet: The backup to the backup)
- 1√ó Transit Gateway ‚Üí eu-central-1 (AWS's internal superhighway)

**AS Number:** 64513 (AWS's second default‚Äîstill no creativity)

**Status:** Mostly idle, occasionally saves the day, always costs money

---

## Circuit Inventory (Or: Where Your Infrastructure Budget Went)

### Dark Fiber Circuits - The "We Own This" Tier

**Circuit Type:** `dark_fiber` (Lit by our equipment, paid for by our CFO's tears)

| Circuit ID | Path | Bandwidth | Status | Purpose |
|------------|------|-----------|--------|---------|
| DF-FRA-001 | DC1 ‚Üí FR5 | 100G | Active | Primary Frankfurt path (The main highway) |
| DF-FRA-002 | DC1 ‚Üí FR5 | 100G | Active | Backup Frankfurt path (Just as fast, twice the reliability) |
| DF-AMS-001 | DC1 ‚Üí AM5 | 100G | Active | Primary Amsterdam path (The scenic route) |
| DF-AMS-002 | DC1 ‚Üí AM5 | 100G | Active | Backup Amsterdam path (When Netherlands fiber cuts happen) |

**Monthly Cost:** You don't want to know. Really.
**Failover Time:** <1 second (BFD is magic)
**Utilization:** 30% average (over-provisioned? No, it's called "headroom")

---

### Direct Connect Circuits - AWS's Toll Roads

**Circuit Type:** `cross_connect` (The short jumper cable that costs $500/month)

| Circuit ID | Path | Bandwidth | Provider | Purpose |
|------------|------|-----------|----------|---------|
| dxcon-fh4a7lqd | FR5 ‚Üí AWS FRA | 10G | AWS | Primary Frankfurt DX (The fast lane) |
| dxcon-fk3b9mpx | FR5 ‚Üí AWS FRA | 10G | AWS | Backup Frankfurt DX (Just as fast, different switch) |
| dxcon-ah7x2npq | AM5 ‚Üí AWS IRL | 10G | AWS | Primary Ireland DX (The Irish Express) |
| dxcon-ak9y3prs | AM5 ‚Üí AWS IRL | 10G | AWS | Backup Ireland DX (Double the cost, double the uptime) |

**Physical Layer:** Metro fiber from Equinix cage to AWS cage (10 meters that cost more than your car payment)
**Logical Layer:** Private VIFs, transit VIFs, and enough BGP sessions to make your head spin
**Latency:** 0.5-2ms (The speed of light is fast, AWS pricing is faster)

---

### Metro Ethernet Circuits - Inter-Colocation Backbone

**Circuit Type:** `metro_ethernet` (Equinix Fabric/Metro Connect‚Äîfancy name for expensive fiber)

| Circuit ID | Path | Bandwidth | Provider | Purpose |
|------------|------|-----------|----------|---------|
| EQX-METRO-001 | FR5 ‚Üî AM5 | 100G | Equinix | Primary inter-colo (The autobahn to tulip country) |
| EQX-METRO-002 | FR5 ‚Üî AM5 | 100G | Equinix | Backup inter-colo (Because even Equinix has bad days) |

**Use Case:** When you need Frankfurt and Amsterdam to share VLANs like they're in the same room
**Latency:** ~3-5ms (350km at the speed of light, plus switch latency)
**Configuration:** Active-active LACP (Because wasting 100G is for people without bandwidth issues)

---

### DIA Circuits - The "Oh Crap" Insurance Policy

**Circuit Type:** `dia` (Dedicated Internet Access‚Äîboring but reliable)

| Circuit ID | Location | Bandwidth | Provider | Purpose |
|------------|----------|-----------|----------|---------|
| DIA-FRA-001 | FR5 | 10G | Technology Partner | Internet breakout + VPN backup (The emergency exit) |
| DIA-AMS-001 | AM5 | 10G | Technology Partner | Internet breakout + VPN backup (The other emergency exit) |

**Why We Need These:**
- When both Direct Connect circuits fail (rare but terrifying)
- When you need internet breakout for SaaS apps (Office 365, we're looking at you)
- When management questions spending on "redundancy" until there's an outage

**Backup VPN:** IPsec tunnels to AWS VGW (20ms latency vs 2ms‚Äîyou'll notice the difference)

---

## Virtual Links (The Overlays Nobody Sees But Everyone Depends On)

### Direct Connect Virtual Interfaces (VIFs)

**Link Type:** `direct_connect_aws` (The expensive toll booth to AWS's private network)

**Frankfurt Direct Connect VIFs:**
- **FR5-to-AWS-FRA-DX-Primary**: Primary path (99.99% of traffic goes here)
- **FR5-to-AWS-FRA-DX-Backup**: Backup path (1% utilization, 100% importance)
- **FR5-to-AWS-IRL-DX-XRegion**: Cross-region failover (For when Frankfurt AWS has issues)

**Amsterdam Direct Connect VIFs:**
- **AM5-to-AWS-IRL-DX-Primary**: Primary path to Ireland
- **AM5-to-AWS-IRL-DX-Backup**: Backup path to Ireland

**Configuration:**
- Private VIFs (not public‚Äîwe're not animals)
- BGP ASN prepending for traffic engineering
- BFD enabled (3-second failure detection)
- BGP communities for fine-grained control

**Encryption:** None (it's a private connection‚Äîif someone's tapping this, you have bigger problems)

---

### IPsec VPN Tunnels (The Internet Backup Plan)

**Link Type:** `vpn_ipsec` (Encrypted shouting matches over the public internet)

**VPN Tunnels:**
- **FR5-to-AWS-FRA-VPN**: Frankfurt ‚Üí AWS Frankfurt over DIA
- **AM5-to-AWS-IRL-VPN**: Amsterdam ‚Üí AWS Ireland over DIA
- **DC1-to-AWS-FRA-VPN**: DC1 ‚Üí AWS Frankfurt (Emergency direct)

**Use Case:** When Direct Connect is down and you need to explain why the monthly AWS bill just doubled
**Latency:** +15-20ms vs Direct Connect
**Throughput:** Limited by DIA circuit (10G) and IPsec overhead (~8G effective)
**Status:** Tested quarterly, relied upon annually, appreciated never

---

### Inter-Colocation Overlays (VXLAN & GRE)

**The VXLAN Squad (Layer 2 Extension):**
- **FR5-AM5-VXLAN-Primary**: Primary VXLAN tunnel (For when you need VLANs everywhere)
- **FR5-AM5-VXLAN-Backup**: Backup VXLAN tunnel (Because VXLAN over redundant 100G links is still not enough)

**The GRE Connection:**
- **FR5-AM5-GRE**: GRE tunnel for routing (When you don't trust VXLAN or just like adding complexity)

**Underlay:** Metro Ethernet circuits (EQX-METRO-001/002)
**Encryption:** None (it's over private circuits‚Äîif Equinix is compromised, we're all screwed)
**Use Case:** Multi-site active-active for databases (bold strategy, let's see if it pays off)

---

### The Global Overlays (Multi-Site Magic)

**Global-SD-WAN:**
- **Type:** `sd_wan` (Marketing buzzword that actually works for once)
- **Spans:** DC1, FR5, AM5, AWS regions (all of them)
- **Managed By:** SD-WAN controller (separate schema, separate headache)
- **Purpose:** When you want application-aware routing and someone in management read a Gartner report

**AWS-FRA-IRL-TGW:**
- **Type:** Transit Gateway (AWS's internal network‚Äîfast and expensive, their specialty)
- **Bandwidth:** 50G (AWS internal backbone‚Äîthey don't mess around)
- **Latency:** ~10-15ms (London datacenters in between make packets do a pub crawl)
- **Use Case:** When you need traffic between AWS regions and Direct Connect
**Use Case:** When you need traffic between AWS regions and Direct Connect isn't enough

---

## Redundancy Model (N+2: Because N+1 is for People Who Enjoy Outages)

### The Redundancy Philosophy

**Management Said:** "We need high availability"
**Engineering Heard:** "Build it so that TWO things can fail simultaneously and nobody notices"
**What We Built:** N+2 at every layer (2 sites, 2 regions, 2 circuits per path, 2 VIFs per circuit)

**Acceptable Failures:** 2 simultaneous failures at the same layer
**Unacceptable Failures:** Your career if you designed N+1

---

### Geographic Redundancy (When Mother Nature Gets Angry)

**2 Colocation Sites:**
- **Frankfurt (FR5)**: German engineering, never fails
- **Amsterdam (AM5)**: Dutch engineering, also never fails
- **Distance:** 350km (Different fiber paths, different earthquakes, different EU countries for legal diversity)

**2 Cloud Regions:**
- **AWS eu-central-1**: Frankfurt (Primary‚Äîfast and expensive)
- **AWS eu-west-1**: Ireland (Secondary‚Äîslower but with better beer)
- **North Sea Between Them:** Natural disaster isolation (Brexit is just a bonus)

**1 Enterprise DC (Dual-Homed):**
- **DC1**: Frankfurt (Your legacy on-premises kingdom)
- **Dual-homed:** To BOTH colocations (Because single points of failure are career-limiting)

---

### Circuit Redundancy (Redundant Redundancy for Redundant Paths)

**Every connection has 2+ physical paths:**

| Connection | Primary | Backup | Emergency | Overkill? |
|------------|---------|--------|-----------|-----------|
| DC ‚Üí FR5 | Dark Fiber 1 | Dark Fiber 2 | VPN | No |
| DC ‚Üí AM5 | Dark Fiber 1 | Dark Fiber 2 | VPN | No |
| FR5 ‚Üí AWS FRA | DX Primary | DX Backup | VPN | Maybe |
| AM5 ‚Üí AWS IRL | DX Primary | DX Backup | VPN | Maybe |
| FR5 ‚Üî AM5 | Metro 1 | Metro 2 | - | Definitely |

**Total Physical Circuits:** 14 (4 dark fiber + 4 Direct Connect + 4 Metro + 2 DIA)
**Total Virtual Links:** 15 (5 VIFs + 3 VPNs + 3 overlays + 2 special + 2 cloud)
**Total Cost:** Classified (CFO cried during budget approval)

---

## Path Diversity (So Many Ways to Get There, You'll Get Lost)

### DC1 ‚Üí AWS Frankfurt (6 Different Paths!)

1. **The Fast Lane**: DC1 ‚Üí FR5 (DF-1) ‚Üí AWS (DX-1) ‚ö° **2ms latency**
2. **The Backup Lane**: DC1 ‚Üí FR5 (DF-2) ‚Üí AWS (DX-2) ‚ö° **2ms latency**
3. **The Scenic Route**: DC1 ‚Üí AM5 (DF-1) ‚Üí FR5 (Metro) ‚Üí AWS (DX-1) üêå **8ms latency**
4. **The Irish Detour**: DC1 ‚Üí AM5 (DF-1) ‚Üí AWS IRL (DX) ‚Üí AWS FRA (TGW) üç∫ **20ms latency**
5. **The Internet Highway**: DC1 ‚Üí FR5 (DF-1) ‚Üí Internet (DIA) ‚Üí AWS (VPN) üåê **18ms latency**
6. **The Direct Panic Button**: DC1 ‚Üí Internet ‚Üí AWS (VPN) üî• **25ms latency**

**Normal Operations:** Path 1 (99% of traffic)
**Single Circuit Failure:** Path 2 (automatic in <1 second)
**FR5 Facility Failure:** Path 3 (automatic via BGP)
**AWS Frankfurt Failure:** Path 4 (manual or automated via health checks)
**Everything on Fire:** Paths 5 or 6 (manual intervention while updating r√©sum√©)

---

### Failure Scenarios (The Real-World Tests Management Doesn't Want to Think About)

#### Scenario 1: Single Circuit Failure
**Failure:** DF-FRA-001 (primary dark fiber) gets cut by construction crew
**Detection:** BFD detects in 3 seconds, BGP withdraws routes
**Failover:** Traffic shifts to DF-FRA-002 automatically
**User Impact:** None (if monitoring isn't watching microsecond jitter)
**Management Reaction:** Oblivious

---

#### Scenario 2: Single PoP Failure
**Failure:** FR5 facility loses power (rare but spectacular)
**Detection:** All circuits from FR5 go down, monitoring explodes
**Failover:** BGP reroutes via AM5 within 30 seconds
**User Impact:** Brief 15-20ms latency spike (AWS traffic goes DC1 ‚Üí AM5 ‚Üí AWS IRL ‚Üí AWS FRA)
**Management Reaction:** "Why are we paying for redundancy?" (Until this happens)

---

#### Scenario 3: Single Cloud Region Failure
**Failure:** AWS eu-central-1 has "Service Event" (AWS-speak for "we messed up")
**Detection:** Health checks fail, CloudWatch alarms scream
**Failover:** Application tier redirects to eu-west-1
**User Impact:** 10-15ms additional latency, some session state loss
**Management Reaction:** Email from AWS apologizing with credits nobody asked for

---

#### Scenario 4: Complete Site Failure (The Big One)
**Failure:** Frankfurt gets hit by meteor/flood/zombie apocalypse
**What Goes Down:** DC1, FR5, AWS eu-central-1 (all in Frankfurt)
**What Stays Up:** AM5 (Amsterdam) + AWS eu-west-1 (Ireland)
**Failover:** Manual intervention required (this is the "CEO calls you at 2 AM" scenario)
**User Impact:** 15-30 minutes of degraded service during DR activation
**Management Reaction:** Suddenly understands why we built N+2

---

#### Scenario 5: Internet-Only Scenario (The Nightmare)
**Failure:** All Direct Connect circuits fail simultaneously (AWS has a REALLY bad day)
**Last Resort:** VPN tunnels over DIA circuits
**Bandwidth:** Drops from 4√ó10G (40G) to 2√ó8G (16G effective after IPsec)
**Latency:** Increases from 2ms to 20ms
**User Impact:** Noticeable but serviceable
**Management Reaction:** "Why are we paying AWS so much?" (Finally asking the right questions)

---

## Files Structure (The Implementation Details You Actually Need)

```
100_flow/
‚îú‚îÄ‚îÄ README.md                    # You are here (congratulations on reading this far)
‚îú‚îÄ‚îÄ 01_infrastructure.yml        # DC1, 2 Colocations, 2 AWS Regions, 6 routers
‚îú‚îÄ‚îÄ 01b_interfaces.yml           # 25 physical + 9 virtual interfaces (the glue that holds it all together)
‚îú‚îÄ‚îÄ 02_circuits.yml              # 14 physical circuits (where money goes to die monthly)
‚îú‚îÄ‚îÄ 03_virtual_links.yml         # 15 virtual links (the software-defined magic)
‚îú‚îÄ‚îÄ 04_routing.yml               # 19 BGP sessions (eBGP, iBGP, and the configs that make it work)
‚îú‚îÄ‚îÄ 05_endpoints.yml             # Test workloads (for when you want to verify this actually works)
‚îú‚îÄ‚îÄ IMPLEMENTATION_PLAN.md       # Technical deep-dive (for the brave)
‚îú‚îÄ‚îÄ TOPOLOGY_VISUALIZATION.md    # ASCII art and diagrams (for visual learners)
‚îú‚îÄ‚îÄ CHECKLIST.md                 # Implementation tracking (for the organized)
‚îî‚îÄ‚îÄ queries/
    ‚îú‚îÄ‚îÄ end_to_end_path.cypher       # Find all paths DC1 ‚Üí AWS
    ‚îú‚îÄ‚îÄ redundancy_check.cypher      # Validate N+2 redundancy
    ‚îî‚îÄ‚îÄ latency_analysis.cypher      # Performance analysis (for optimization nerds)
```

---

## Enhanced Connectivity Map (The Big Picture)

```
                          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
                          ‚ïë      DC1 (Frankfurt) AS 65000     ‚ïë
                          ‚ïë  2√ó Border Routers                 ‚ïë
                          ‚ïë  Legacy Apps + Compliance Zone     ‚ïë
                          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
                                         ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ                              ‚îÇ
              [Dark Fiber 100G √ó 2]          [Dark Fiber 100G √ó 2]
                          ‚îÇ                              ‚îÇ
                          ‚Üì                              ‚Üì
        ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
        ‚ïë  FR5 (Frankfurt) AS 65100  ‚ïë‚Üê‚îÄ‚îÄ‚Üí‚ïë  AM5 (Amsterdam) AS 65101 ‚ïë
        ‚ïë  Equinix Colocation        ‚ïë    ‚ïë  Equinix Colocation       ‚ïë
        ‚ïë  2√ó Edge Routers           ‚ïë    ‚ïë  2√ó Edge Routers          ‚ïë
        ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
                    ‚îÇ                                  ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ             ‚îÇ              ‚îÇ      ‚îÇ           ‚îÇ              ‚îÇ
   [DX 10G]    [DIA 10G]  [Metro 100G√ó2] [DX 10G] [DIA 10G]         ‚îÇ
      ‚îÇ             ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ              ‚îÇ
      ‚îÇ          [VPN]                              [VPN]             ‚îÇ
      ‚îÇ             ‚îÇ                                  ‚îÇ              ‚îÇ
      ‚Üì             ‚Üì                                  ‚Üì              ‚îÇ
  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó                  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
  ‚ïë AWS eu-central-1      ‚ïë                  ‚ïë AWS eu-west-1          ‚ïë
  ‚ïë AS 64512 (Frankfurt)  ‚ïë‚Üê‚îÄ‚îÄ[Transit GW]‚îÄ‚îÄ‚Üí‚ïë AS 64513 (Ireland)     ‚ïë
  ‚ïë VPC + DX Gateway      ‚ïë                  ‚ïë VPC + DX Gateway       ‚ïë
  ‚ïë Primary Production    ‚ïë                  ‚ïë DR + Failover          ‚ïë
  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù                  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

**Legend:**
- **Dark Fiber**: Expensive but fast (the Autobahn of data)
- **Direct Connect (DX)**: AWS's premium toll road
- **DIA**: Dedicated Internet Access (the humble insurance policy)
- **VPN**: Encrypted internet tunnels (slow but reliable)
- **Metro**: Inter-colocation backbone (Equinix's money printer)
- **Transit GW**: AWS inter-region superhighway

---

## BGP Configuration (The Routing Glue That Makes It All Work)

### BGP AS Numbers

| Location | AS Number | Type | Personality |
|----------|-----------|------|-------------|
| DC1 | 65000 | Private | The mothership (originating 10.0.0.0/8) |
| FR5 | 65100 | Private | The German transit hub |
| AM5 | 65101 | Private | The Dutch backup hub |
| AWS FRA | 64512 | Private | AWS default (because creativity is hard) |
| AWS IRL | 64513 | Private | AWS second default (still no creativity) |

### BGP Sessions Summary

**eBGP Sessions (Between Different AS):**
- DC1 (65000) ‚Üî FR5 (65100): 2 sessions (primary + backup circuit)
- DC1 (65000) ‚Üî AM5 (65101): 2 sessions (primary + backup circuit)
- FR5 (65100) ‚Üî AWS FRA (64512): 3 sessions (2 DX + 1 cross-region)
- AM5 (65101) ‚Üî AWS IRL (64513): 3 sessions (2 DX + 1 VPN)
- DC1 (65000) ‚Üî Internet: 2 sessions (DIA circuits with default route)

**iBGP Sessions (Within Same AS or Special Cases):**
- FR5 ‚Üî AM5: 2 sessions (if peering AS, confederations, or just because)
- DC1 internal: Route reflectors (for internal redundancy)

**Total BGP Sessions:** 19+ (enough to keep network engineers employed)

### Traffic Engineering Tricks

**AS Path Prepending:**
- Prefer Direct Connect over VPN: Prepend VPN routes 3x
- Prefer primary over backup: Prepend backup routes 2x
- Prefer Frankfurt over Ireland: Prepend Ireland routes 1x

**BGP Communities:**
- `65000:100` = Primary production traffic
- `65000:200` = Backup traffic (use only if primary fails)
- `65000:666` = Do not advertise to peers (local only)
- `65000:999` = Emergency override (manual intervention)

**BFD (Bidirectional Forwarding Detection):**
- Interval: 300ms
- Multiplier: 3√ó (detect failure in 900ms)
- Result: Sub-second failover (when it works)

---

## Quick Start Guide (For the Impatient)

### Prerequisites (Read This or Suffer)

1. **InfraHub Running:** Obviously. `./r.sh` if you need to start fresh.
2. **Schemas Loaded:** TopologyCircuit and TopologyVirtualLink schemas exist on branch.
3. **Coffee:** Strong. This is a 30-minute demo, not a 5-minute one.
4. **Patience:** Management might question the bill, have justifications ready.

### Step 1: Create Branch (Isolation is Good)

```bash
# Create a new branch for this topology flow
uv run infrahubctl branch create flow

# Verify branch exists (paranoia is healthy)
uv run infrahubctl branch list
```

### Step 2: Load Infrastructure (The Foundation)

```bash
# Load topologies: DC1, Equinix FR5/AM5, AWS regions
uv run infrahubctl object load data/demos/100_flow/01_infrastructure.yml --branch flow

# Verify devices created
uv run infrahubctl get DcimPhysicalDevice --branch flow | grep -E "dc1-border|fr5-edge|am5-edge"
```

**Expected Output:** 6 routers (2 DC1 border + 2 FR5 edge + 2 AM5 edge)

### Step 3: Load Interfaces (The Connection Points)

```bash
# Load 25 physical + 9 virtual interfaces
uv run infrahubctl object load data/demos/100_flow/01b_interfaces.yml --branch flow

# Verify interfaces created (should see 34 total)
uv run infrahubctl get DcimPhysicalInterface --branch flow | wc -l
uv run infrahubctl get DcimVirtualInterface --branch flow | wc -l
```

**Expected Output:** 25 physical (Ethernet + Loopback) + 9 virtual (Tunnel + Vxlan + GRE) = 34 interfaces

### Step 4: Load Physical Cables (The Actual Wires)

```bash
# Load 4 physical cables (dark fiber only - DC to colocation)
# Note: Equinix Metro Connect (FR5 ‚Üî AM5) is a Layer 2 service, NOT a cable
# Note: Direct Connect cross-connects and DIA circuits handled by circuit bindings only
uv run infrahubctl object load data/demos/100_flow/01c_cables.yml --branch flow

# Verify cables created
uv run infrahubctl get DcimCable --branch flow
```

**Expected Output:** 4 cables connecting DC to colocations:
- 2√ó DC1 ‚Üî FR5 (dark fiber - owned/leased fiber)
- 2√ó DC1 ‚Üî AM5 (dark fiber - owned/leased fiber)

**Important Distinctions:**
- **Physical cables** = Direct fiber you own/lease (DC ‚Üî Colocation)
- **Equinix Metro Connect** = Layer 2 service (circuit only, no cable)
- **Direct Connect cross-connects** = Terminate in AWS cages (circuit service binding only)
- **DIA circuits** = Terminate in ISP PoPs (circuit service binding only)

### Step 5: Load Circuits (The Physical Connectivity Layer)

```bash
# Load 14 physical circuits (dark fiber, DX cross-connects, metro, DIA)
uv run infrahubctl object load data/demos/100_flow/02_circuits.yml --branch flow

# Verify circuits created
uv run infrahubctl get TopologyCircuit --branch flow
```

**Expected Output:** 14 circuits with interface bindings

### Step 6: Load Virtual Links (The Logical Overlay)

```bash
# Load 15 virtual links (VIFs, VPNs, VXLAN, GRE, SD-WAN, TGW)
uv run infrahubctl object load data/demos/100_flow/03_virtual_links.yml --branch flow

# Verify virtual links created
uv run infrahubctl get TopologyVirtualLink --branch flow
```

**Expected Output:** 15 virtual links with endpoint and deployment bindings

### Step 7: Validate Complete Topology

```bash
# Check all components loaded
echo "=== Topology Summary ==="
echo "Devices: $(uv run infrahubctl get DcimPhysicalDevice --branch flow | wc -l)"
echo "Interfaces: $(uv run infrahubctl get DcimPhysicalInterface --branch flow | wc -l)"
echo "Cables: $(uv run infrahubctl get DcimCable --branch flow | wc -l)"
echo "Circuits: $(uv run infrahubctl get TopologyCircuit --branch flow | wc -l)"
echo "Virtual Links: $(uv run infrahubctl get TopologyVirtualLink --branch flow | wc -l)"
```

**Expected Output:**
- Devices: 6
- Interfaces: 25+ (physical only, virtual interfaces are separate kind)
- Cables: 4 (DC to colocation dark fiber only)
- Circuits: 14 (includes dark fiber, Metro Connect, cross-connects, DIA)
- Virtual Links: 15

---

## CYPHER Visualization Queries (The Fun Part)

### Updated for Complete Stack: Cables ‚Üí Circuits ‚Üí Services ‚Üí Virtual Links

All CYPHER queries have been updated to include:
- ‚úÖ **Physical cables** (DcimCable)
- ‚úÖ **Unified connectivity** (TopologyConnectivity generic)
- ‚úÖ **Service bindings** (ManagedCircuitService, ManagedVirtualLinkService)
- ‚úÖ **Full layer traversal** (Cable ‚Üí Circuit ‚Üí Virtual Link)

### Query 1: Complete End-to-End Path Discovery

```cypher
// Shows all layers from physical cables to virtual links
MATCH path = (dc:Node {kind: "TopologyDataCenter", name: "DC1"})
  -[:HAS_ATTRIBUTE__CONNECTIVITY*1..10]-
  (cloud:Node {kind: "TopologyCloudRegion", name: "eu-central-1"})
RETURN path
ORDER BY length(path)
LIMIT 10;
```

**What This Shows:** Complete paths including all connectivity layers

**Location:** `queries/end_to_end_path.cypher` (7 comprehensive queries)

### Query 2: Physical Cable Layer

```cypher
// Shows actual fiber connections between devices
MATCH (cable:Node {kind: "DcimCable"})
OPTIONAL MATCH (cable)-[:HAS_ATTRIBUTE__ENDPOINTS]->(endpoint:Node {kind: "DcimPhysicalInterface"})
OPTIONAL MATCH (endpoint)-[:IS_RELATED__DEVICE]->(device:Node {kind: "DcimPhysicalDevice"})
RETURN cable.name AS CableName,
       cable.type AS CableType,
       collect(DISTINCT device.name) AS ConnectedDevices
ORDER BY cable.name;
```

**What This Shows:** Physical fiber between DC1 and colocation routers

### Query 3: Redundancy Check Across All Layers

```cypher
// Comprehensive redundancy validation
MATCH (dc:Node {kind: "TopologyDataCenter", name: "DC1"})
OPTIONAL MATCH (dc)-[:IS_RELATED__CABLES]->(cable:Node {kind: "DcimCable"})
OPTIONAL MATCH (dc)-[:HAS_ATTRIBUTE__CONNECTIVITY]->(circuit:Node {kind: "TopologyCircuit"})
OPTIONAL MATCH (dc)-[:HAS_ATTRIBUTE__CONNECTIVITY]->(vlink:Node {kind: "TopologyVirtualLink"})
WITH dc,
     count(DISTINCT cable) AS cables,
     count(DISTINCT circuit) AS circuits,
     count(DISTINCT vlink) AS vlinks
RETURN dc.name AS Deployment,
       cables AS PhysicalCables,
       circuits AS Circuits,
       vlinks AS VirtualLinks,
       CASE
         WHEN cables >= 4 AND circuits >= 4 AND vlinks >= 4 THEN "‚úì‚úì Highly redundant"
         WHEN cables >= 2 AND circuits >= 2 AND vlinks >= 2 THEN "‚úì Redundant"
         ELSE "‚ö† Partial redundancy"
       END AS OverallStatus;
```

**What This Shows:** Redundancy status at every layer (cables, circuits, virtual links)

**Location:** `queries/redundancy_check.cypher` (8 redundancy queries)

### Query 4: Latency Analysis with Physical Layer

```cypher
// Calculate realistic latency including cable propagation
MATCH path = (dc:Node {kind: "TopologyDataCenter", name: "DC1"})
  -[:HAS_ATTRIBUTE__CONNECTIVITY*1..10]-
  (cloud:Node {kind: "TopologyCloudRegion", name: "eu-central-1"})
WITH path,
     [node IN nodes(path) WHERE node.kind IN ["TopologyCircuit", "TopologyVirtualLink"] | node] AS links
UNWIND links AS link
WITH path,
     CASE link.circuit_type
       WHEN "dark_fiber" THEN 1.5      // ~1.5ms per 300km
       WHEN "cross_connect" THEN 0.5   // <1ms
       WHEN "metro_ethernet" THEN 3.0  // ~3ms service overhead
       ELSE 5.0
     END AS circuit_latency,
     CASE link.link_type
       WHEN "direct_connect_aws" THEN 2.0      // 1-3ms
       WHEN "vxlan" THEN 0.5                   // minimal encap
       WHEN "vpn_ipsec" THEN 15.0              // crypto overhead
       ELSE 3.0
     END AS vlink_latency
WITH path,
     sum(coalesce(circuit_latency, 0)) + sum(coalesce(vlink_latency, 0)) AS total_latency_ms
RETURN round(total_latency_ms * 10) / 10 AS Latency_ms,
       CASE
         WHEN total_latency_ms < 5 THEN "‚úì‚úì Excellent"
         WHEN total_latency_ms < 10 THEN "‚úì Good"
         WHEN total_latency_ms < 20 THEN "‚óã Acceptable"
         ELSE "‚ö† High"
       END AS LatencyClass
ORDER BY total_latency_ms ASC
LIMIT 5;
```

**What This Shows:** Real-world latency estimates including physical propagation delay

**Location:** `queries/latency_analysis.cypher` (6 performance queries)

### Query 5: Service Binding View

```cypher
// Shows which services are bound to which interfaces
MATCH (device:Node {kind: "DcimPhysicalDevice"})
  -[:IS_RELATED__INTERFACES]->(iface:Node {kind: "DcimPhysicalInterface"})
  -[:IS_RELATED__INTERFACE_SERVICES]->(service:Node)
WHERE service.kind IN ["ManagedCircuitService", "ManagedVirtualLinkService"]
OPTIONAL MATCH (service)-[:IS_RELATED__CIRCUIT]->(circuit:Node {kind: "TopologyCircuit"})
OPTIONAL MATCH (service)-[:IS_RELATED__VIRTUAL_LINK]->(vlink:Node {kind: "TopologyVirtualLink"})
RETURN device.name AS Device,
       iface.name AS Interface,
       service.kind AS ServiceType,
       coalesce(circuit.circuit_id, vlink.name, "N/A") AS BoundTo
ORDER BY device.name, iface.name;
```

**What This Shows:** Interface-level service bindings for configuration generation

### Query Files Summary

| File | Queries | Purpose |
|------|---------|---------|
| **end_to_end_path.cypher** | 7 queries | Path discovery at all layers (cables, circuits, services, virtual links) |
| **redundancy_check.cypher** | 8 queries | Validate N+1/N+2 redundancy, find SPOFs, check component status |
| **latency_analysis.cypher** | 6 queries | Performance optimization, bandwidth analysis, path ranking |

**All queries updated to support:**
- Physical cable layer (DcimCable)
- Unified connectivity relationship (deployment.connectivity)
- Service bindings (ManagedCircuitService, ManagedVirtualLinkService)
- Multi-layer traversal (Physical ‚Üí Circuit ‚Üí Virtual)

---

## Path Analysis Examples

### Example 1: Dark Fiber Path (DC1 ‚Üí FR5 ‚Üí AWS)
  length(path) AS hops,
  cloud.name AS destination
ORDER BY hops, destination
```

**Expected Results:**
- Path 1: DC1 ‚Üí FR5 ‚Üí AWS Frankfurt (2 hops)
- Path 2: DC1 ‚Üí AM5 ‚Üí AWS Ireland (2 hops)
- Path 3: DC1 ‚Üí FR5 ‚Üí AM5 ‚Üí AWS * (3 hops)
- Path 4+: Combinations with failovers and backup paths

---

### Query 2: Redundancy Validation

```cypher
// Validate N+2 redundancy: Every connection has 2+ paths
MATCH (source)-[r:CONNECTED_VIA]->(destination)
WITH source, destination, count(r) AS path_count
WHERE path_count < 2
RETURN
  source.name AS from,
  destination.name AS to,
  path_count AS paths,
  "REDUNDANCY FAILURE" AS status
```

**Expected Results:** Empty (if everything is configured correctly)
**If Not Empty:** You found a single point of failure‚Äîfix it before management finds out

---

### Query 3: Latency Analysis

```cypher
// Compare path latencies for optimization
MATCH path = (dc:TopologyDataCenter)-[:CONNECTED_VIA*]->(cloud:TopologyCloudRegion)
RETURN
  dc.name AS source,
  cloud.name AS destination,
  [node IN nodes(path) | node.name] AS path_hops,
  length(path) AS hop_count,
  // Estimated latency (simplified)
